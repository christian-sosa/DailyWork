{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import awswrangler as wr\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_partition_fields(df, formato=\"%Y\", col=\"ano\"):\n",
    "    df[\"year\"] = [x for x in df[col]]\n",
    "    return df\n",
    "\n",
    "def df_tasks(df, tasks):\n",
    "    if (\n",
    "        \"del\" in tasks\n",
    "    ):  # Esta porcion de codigo, borra las filas de las columnas especificadas\n",
    "        columns = tasks[\"del\"]\n",
    "        columns = columns.split(\",\")\n",
    "        print(\"columnas: \", columns)\n",
    "        if len(columns) > 0:\n",
    "            df2 = df[\n",
    "                df.tipo_plp != \"X\"\n",
    "            ]  # Elimino todas las filas que contengan X en la columna tipo_plp\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamoDBTable = os.environ[\"dynamoDBTable\"]\n",
    "element = os.environ[\"element\"]\n",
    "\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "table = dynamodb.Table(dynamoDBTable)\n",
    "\n",
    "config = table.get_item(Key={\"CEN\": \"{}\".format(element)}).get(\"Item\")\n",
    "data = config[\n",
    "    \"params\"\n",
    "]  # Dentro de \"params\" se encuentran todos los parámetros necesarios para trabajar con la tabla antes listada.\n",
    "data_params = json.loads(data)  # Conversión de json a lista\n",
    "\n",
    "# Base de datos del catálogo de glue a escribir:\n",
    "database = data_params.get(\"database_analytics\")\n",
    "# Ruta de origen del archivo a leer:\n",
    "S3_origin = data_params.get(\"S3_origin_analytics\")\n",
    "# Ruta de destino del archivo a escribir:\n",
    "S3_dest = data_params.get(\"S3_dest_analytics\")\n",
    "# Extension del archivo a leer\n",
    "format_in = data_params.get(\"format_in\")\n",
    "# Extension del archivo a escribir\n",
    "format_out = data_params.get(\"format_out\")\n",
    "# Tipo de escritura: (overwrite_partitions, overwrite, append)\n",
    "write_mode = data_params.get(\"write_mode\")\n",
    "# FACT\n",
    "element = \"fact_\" + element\n",
    "# Tareas a realizar sobre el dataframe\n",
    "tasks = data_params.get(\"tasks\")\n",
    "tasks = tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = wr.s3.read_parquet(path=S3_origin)\n",
    "df1 = df_tasks(df1, tasks)\n",
    "\n",
    "df2 = df1[\"escenario\"]\n",
    "df2.drop_duplicates(inplace=True)\n",
    "dict = df2.to_dict()\n",
    "escenarios = list(dict.values())\n",
    "\n",
    "df3 = df1[\"region\"]\n",
    "df3.drop_duplicates(inplace=True)\n",
    "dict = df3.to_dict()\n",
    "regiones = list(dict.values())\n",
    "\n",
    "df4 = df1[\"tecnologia\"]\n",
    "df4.drop_duplicates(inplace=True)\n",
    "dict = df4.to_dict()\n",
    "tecnologias = list(dict.values())\n",
    "print(tecnologias)\n",
    "\n",
    "df5 = df1[\"ano\"]\n",
    "df5.drop_duplicates(inplace=True)\n",
    "anho_min = df5.min()\n",
    "anho_max = df5.max()\n",
    "\n",
    "for i in escenarios:\n",
    "    for x in regiones:\n",
    "        for y in tecnologias:\n",
    "            for anho in range(anho_min, anho_max + 1):\n",
    "\n",
    "                if df1.loc[\n",
    "                    (df1[\"escenario\"] == i)\n",
    "                    & (df1[\"region\"] == x)\n",
    "                    & (df1[\"tecnologia\"] == y)\n",
    "                    & (df1[\"ano\"] == anho)\n",
    "                ].empty:\n",
    "                    new_row = {\n",
    "                        \"escenario\": i,\n",
    "                        \"region\": x,\n",
    "                        \"tecnologia\": y,\n",
    "                        \"potencia\": 0,\n",
    "                        \"ano\": anho,\n",
    "                    }\n",
    "                    df1 = df1.append(new_row, ignore_index=True)\n",
    "df1 = df1.groupby(\n",
    "    [\"escenario\", \"region\", \"tecnologia\", \"ano\"], as_index=False\n",
    ").sum()\n",
    "df2 = add_partition_fields(df1)\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "            df=df2,\n",
    "            path=S3_dest,\n",
    "            index=False,\n",
    "            dataset=True,\n",
    "            database=database,\n",
    "            table=element,\n",
    "            compression=\"gzip\",\n",
    "            mode=write_mode,\n",
    "            partition_cols=[\"year\"],\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
